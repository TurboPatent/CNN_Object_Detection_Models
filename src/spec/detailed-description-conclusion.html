<p>
  <b>Region-based Fully Convolutional Network&nbsp;(R-FCN)</b>
</p>
<p>Fast and Faster R-CNN methodologies consist in detecting region proposals and recognize an object in each region. The Region-based Fully Convolutional Network (R-FCN)&nbsp;is a model with only convolutional layers allowing complete backpropagation for training and inference. The method merged the two basic steps in a single model to take into account simultaneously the object detection (location invariant) and its position (location variant).</p>
<p>A&nbsp;ResNet-101&nbsp;model takes the initial image as input. The last layer outputs feature maps, each one is specialized in the detection of a category at some location. For example, one feature map is specialized in the detection of a cat, another one in a banana and so on. Such feature maps are called&nbsp;position-sensitive score maps&nbsp;because they take into account the spatial localization of a particular object. It consists of&nbsp;k*k*(C+1)&nbsp;score maps where k is the size of the score map, and C the number of classes. All these maps form the score bank. Basically, we create patches that can recognize part of an object. For example, for&nbsp;k=3, we can recognize 3x3 parts of an object.</p>
<p>In parallel, the method runs a RPN to generate Region of Interest (RoI). Finally, the method cuts each RoI in bins and checks them against the score bank. If enough of these parts are activated, then the patch vote ‘yes’, I recognized the object.</p>
<p>
  <b>You Only Look Once&nbsp;(YOLO)</b>
</p>
<p>The YOLO model&nbsp;directly predicts bounding boxes and class probabilities with a single network in a single evaluation. The simplicity of the YOLO model allows real-time predictions.</p>
<p>Initially, the model takes an image as input. It divides it into an SxS grid. Each cell of this grid predicts B bounding boxes with a confidence score. This confidence is simply the probability to detect the object multiply by the IoU between the predicted and the ground truth boxes.</p>
<p>The CNN used is inspired by the&nbsp;GoogLeNet&nbsp;model introducing the inception modules. The network has 24 convolutional layers followed by 2 fully-connected layers. Reduction layers with 1x1 filters⁴ followed by 3x3 convolutional layers replace the initial inception modules. The Fast YOLO model is a lighter version with only 9 convolutional layers and fewer number of filters. Most of the convolutional layers are pretrained using the ImageNet dataset with classification. Four convolutional layers followed by two fully-connected layers are added to the previous network and it is entirely retrained with the PASCAL VOC datasets.</p>
<p>The final layer outputs a&nbsp;S*S*(C+B*5)&nbsp;tensor corresponding to the predictions for each cell of the grid. C is the number of estimated probabilities for each class. B is the fixed number of anchor boxes per cell, each of these boxes being related to 4 coordinates (coordinates of the center of the box, width and height) and a confidence value.</p>
<p>With the previous models, the predicted bounding boxes often contained an object. The YOLO model however predicts a high number of bounding boxes. Thus there are a lot of bounding boxes without any object. The&nbsp;Non-Maximum Suppression&nbsp;(NMS) method is applied at the end of the network. It consists in merging highly-overlapping bounding boxes of a same object into a single one.</p>
<p>
  <b>Single-Shot Detector&nbsp;(SSD)</b>
</p>
<p>A Single-Shot Detector (SSD) model predicts all at once the bounding boxes and the class probabilities with an end-to-end CNN architecture.</p>
<p>The model takes an image as the input which passes through multiple convolutional layers with different sizes of filter (10x10, 5x5 and 3x3). Feature maps from convolutional layers at different position of the network are used to predict the bounding boxes. They are processed by a specific convolutional layers with 3x3 filters called extra feature layers to produce a set of bounding boxes similar to the anchor boxes of the Fast R-CNN.</p>
<p>Each box has 4 parameters: the coordinates of the center, the width and the height. At the same time, it produces a vector of probabilities corresponding to the confidence over each class of object.</p>
<p>The Non-Maximum Suppression method is also used at the end of the SSD model to keep the most relevant bounding boxes. The&nbsp;Hard Negative Mining (HNM) is then used because a lot of negative boxes are still predicted. It consists in selecting only a subpart of these boxes during the training. The boxes are ordered by confidence and the top is selected depending on the ratio between the negative and the positive which is at most 1/3.</p>
<p>
  <b>Neural Architecture Search Net&nbsp;(NASNet)</b>
</p>
<p>The Neural Architecture Search&nbsp;consists in learning the architecture of a model to optimize the number of layers while improving the accuracy over a given dataset.</p>
<p>The NASNet network has an architecture learned from the CIFAR-10 dataset and is trained with the ImageNet dataset. This model is used for feature maps generation and is stacked into the Faster R-CNN pipeline. Then the entire pipeline is retrained with the COCO dataset.</p>
<p>
  <b>Mask Region-based Convolutional Network (Mask&nbsp;R-CNN)</b>
</p>
<p>Another extension of the Faster R-CNN model adds a parallel branch to the bounding box detection in order to predict object mask. The mask of an object is its segmentation by pixel in an image. This model outperforms the state-of-the-art in the four COCO challenges: the instance segmentation, the bounding box detection, the object detection and the key point detection.</p>
<p>The Mask Region-based Convolutional Network (Mask R-CNN) uses the Faster R-CNN pipeline with three output branches for each candidate object: a class label, a bounding box offset and the object mask. It uses Region Proposal Network (RPN) to generate bounding box proposals and produces the three outputs at the same time for each Region of Interest (RoI).</p>
<p>The initial RoIPool layer used in the Faster R-CNN is replaced by a&nbsp;RoIAlign layer. It removes the quantization of the coordinates of the original RoI and computes the exact values of the locations. The RoIAlign layer provides scale-equivariance and translation-equivariance with the region proposals.</p>
<p>The model takes an image as input and feeds a&nbsp;ResNeXt&nbsp;network with 101 layers. This model looks like a ResNet but each residual block is cut into lighter transformations which are aggregated to add sparsity in the block. The model detects RoIs which are processed using a RoIAlign layer. One branch of the network is linked to a fully-connected layer to compute the coordinates of the bounding boxes and the probabilities associated to the objects. The other branch is linked to two convolutional layers, the last one computes the mask of the detected object.</p>
<p>Three loss functions associated to each task to solve are summed. This sum is minimized and produces great performances because solving the segmentation task improve the localization and thus the classification.</p>